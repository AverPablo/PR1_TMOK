# -*- coding: utf-8 -*-
"""02_Standardizing_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gsAK84WtrJ5RSRLji0uCytIjkVNad_WI

# Standardizing Data
"""

import pandas as pd
import numpy as np

"""## Standardizing Data
- Standardization
    - Preprocessing method used to transform continuous data to make it look normally distributed
    - Scikit-learn models assume normally distributed data
        - Log normalization
        - feature Scaling
- When to standardize: models
    - Model in linear space
    - Dataset features have high variance
    - Dataset features are continuous and on different scales
    - Linearity assumptions

### Modeling without normalizing
Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, `Proline`, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.

The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (`knn`) as well as the `X` and `y` sets you need to fit and score on.
"""

wine = pd.read_csv('wine_types.csv')  # Читаем CSV файл 'wine_types.csv' и загружаем его содержимое в переменную 'wine' в виде DataFrame.
wine.head()  # Выводим первые 5 строк DataFrame 'wine' для предварительного просмотра данных.

X = wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]  # Извлекаем столбцы 'Proline', 'Total phenols', 'Hue' и 'Nonflavanoid phenols' из DataFrame 'wine' и сохраняем их в переменной 'X' как набор признаков (features).
y = wine['Type']  # Извлекаем столбец 'Type' из DataFrame 'wine' и сохраняем его в переменной 'y' как целевую переменную (target).

from sklearn.model_selection import train_test_split  # Импортируем функцию train_test_split из модуля model_selection библиотеки scikit-learn для разделения данных на обучающую и тестовую выборки.
from sklearn.neighbors import KNeighborsClassifier  # Импортируем класс KNeighborsClassifier из модуля neighbors библиотеки scikit-learn для создания модели классификации на основе метода k-ближайших соседей.

knn = KNeighborsClassifier()  # Создаем экземпляр модели KNeighborsClassifier, который будет использоваться для классификации.

# Разделяем набор данных и метки на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y)  # Используем функцию train_test_split для разделения признаков (X) и целевой переменной (y) на обучающую выборку (X_train, y_train) и тестовую выборку (X_test, y_test).

# Обучаем модель k-ближайших соседей на обучающих данных
knn.fit(X_train, y_train)  # Обучаем модель knn на обучающих данных, используя метод fit, который принимает обучающие признаки и метки.

# Оцениваем модель на тестовых данных
print(knn.score(X_test, y_test))  # Выводим точность модели на тестовых данных, используя метод score, который возвращает долю правильных предсказаний.

"""## Log normalization
- Applies log transformation
- Natural log using the constant $e$ (2.718)
- Captures relative changes, the magnitude of change, and keeps everything in the positive space

### Checking the variance
Check the variance of the columns in the `wine` dataset.
"""

wine.describe()
# Вызываем метод describe() для объекта DataFrame wine
# Этот метод возвращает описательную статистику для всех числовых столбцов в DataFrame

"""The `Proline` column has an extremely high variance.

### Log normalization in Python
Now that we know that the `Proline` column in our wine dataset has a large amount of variance, let's log normalize it.
"""

# Выводим дисперсию столбца Proline
print(wine['Proline'].var())

# Применяем логарифмическую нормализацию к столбцу Proline
wine['Proline_log'] = np.log(wine['Proline'])

# Проверяем дисперсию нормализованного столбца Proline
print(wine['Proline_log'].var())

"""## Scaling data for feature comparison
- Features on different scales
- Model with linear characteristics
- Center features around 0 and transform to unit variance(1)
- Transforms to approximately normal distribution

### Scaling data - investigating columns
We want to use the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the `wine` dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. Using `describe()` to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?
"""

# Вызываем метод describe() для выборки столбцов Ash, Alcalinity of ash и Magnesium из DataFrame wine
# Этот метод возвращает описательную статистику для указанных числовых столбцов
wine[['Ash', 'Alcalinity of ash', 'Magnesium']].describe()

"""### Scaling data - standardizing columns
Since we know that the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the `wine` dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.


"""

from sklearn.preprocessing import StandardScaler

# Создаем объект StandardScaler для стандартизации данных
ss = StandardScaler()

# Берем подмножество DataFrame, которое мы хотим стандартизировать
wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]

# Выводим первые три строки подмножества для проверки
print(wine_subset.iloc[:3])

# Применяем стандартизатор к подмножеству DataFrame
wine_subset_scaled = ss.fit_transform(wine_subset)

# Выводим первые три строки стандартизированного подмножества
print(wine_subset_scaled[:3])

"""## Standardized data and modeling

### KNN on non-scaled data
Let's first take a look at the accuracy of a K-nearest neighbors model on the `wine` dataset without standardizing the data. The `knn` model as well as the `X` and `y` data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you.
"""

# Импортируем библиотеку pandas для работы с данными
import pandas as pd

# Загружаем данные из CSV файла 'wine_types.csv' в DataFrame
wine = pd.read_csv('wine_types.csv')

# Удаляем столбец 'Type' из DataFrame и сохраняем оставшиеся данные в переменной X
X = wine.drop('Type', axis=1)

# Извлекаем столбец 'Type' из DataFrame и сохраняем его в переменной y
y = wine['Type']

# Импортируем класс KNeighborsClassifier из библиотеки sklearn для классификации
from sklearn.neighbors import KNeighborsClassifier

# Создаем экземпляр классификатора KNN
knn = KNeighborsClassifier()

# Импортируем функцию train_test_split из библиотеки sklearn для разделения данных
from sklearn.model_selection import train_test_split

# Разделяем набор данных и метки на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Обучаем модель K-ближайших соседей на обучающих данных
knn.fit(X_train, y_train)

# Оцениваем точность модели на тестовых данных и выводим результат
print(knn.score(X_test, y_test))

"""### KNN on scaled data
The accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data.


"""

# Импортируем класс KNeighborsClassifier из библиотеки sklearn для классификации
from sklearn.neighbors import KNeighborsClassifier

# Создаем экземпляр модели K-ближайших соседей
knn = KNeighborsClassifier()

# Создаем метод масштабирования
ss = StandardScaler()

# Применяем метод масштабирования к набору данных, используемому для моделирования
X_scaled = ss.fit_transform(X)

# Разделяем масштабированные данные и метки на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)

# Обучаем модель K-ближайших соседей на обучающих данных
knn.fit(X_train, y_train)

# Оцениваем точность модели на тестовых данных и выводим результат
print(knn.score(X_test, y_test))